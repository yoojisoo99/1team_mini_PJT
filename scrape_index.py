"""
ğŸ“ˆ KOSPI / KOSDAQ ì¼ë³„ ì§€ìˆ˜ ìŠ¤í¬ë˜í•‘
====================================
ë„¤ì´ë²„ ê¸ˆìœµ ì¼ë³„ ì‹œì„¸ í˜ì´ì§€ì—ì„œ ì§€ìˆ˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
  - requests + BeautifulSoup ì‚¬ìš©
  - data/market_index_YYYYMMDD.csv ë¡œ ì €ì¥
"""

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import os
import time
import logging

# ============================================================
# ë¡œê¹… ì„¤ì •
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# ============================================================
# ê³µí†µ ì„¤ì •
# ============================================================
HEADERS = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) '
                  'AppleWebKit/537.36 (KHTML, like Gecko) '
                  'Chrome/120.0.0.0 Safari/537.36'
}

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data')
os.makedirs(DATA_DIR, exist_ok=True)


def create_session(retries=3, backoff=0.5):
    """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ requests.Sessionì„ ìƒì„±í•©ë‹ˆë‹¤."""
    session = requests.Session()
    retry_strategy = Retry(
        total=retries,
        backoff_factor=backoff,
        status_forcelist=[429, 500, 502, 503, 504],
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    session.headers.update(HEADERS)
    return session


def scrape_index_daily(code="KOSPI", pages=10, session=None):
    """
    ë„¤ì´ë²„ ê¸ˆìœµ ì¼ë³„ ì‹œì„¸ í˜ì´ì§€ì—ì„œ ì§€ìˆ˜ ë°ì´í„°ë¥¼ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤.

    Args:
        code: "KOSPI" ë˜ëŠ” "KOSDAQ"
        pages: ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ â‰ˆ 10ì¼, 10í˜ì´ì§€ â‰ˆ 100ì¼)
        session: requests.Session (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
    Returns:
        pandas DataFrame (Date, Close, ì „ì¼ë¹„, ë“±ë½ë¥ , ê±°ë˜ëŸ‰, ê±°ë˜ëŒ€ê¸ˆ, ì‹œì¥)
    """
    if session is None:
        session = create_session()

    base_url = f"https://finance.naver.com/sise/sise_index_day.naver?code={code}"
    all_rows = []

    logger.info(f"[ì§€ìˆ˜ ìˆ˜ì§‘] {code} ì¼ë³„ ì‹œì„¸ ìˆ˜ì§‘ ì‹œì‘ ({pages}í˜ì´ì§€)")

    for page in range(1, pages + 1):
        url = f"{base_url}&page={page}"
        try:
            resp = session.get(url, timeout=10)
            resp.encoding = 'euc-kr'
            soup = BeautifulSoup(resp.text, 'html.parser')
        except requests.RequestException as e:
            logger.error(f"  [ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜] {code} page={page}: {e}")
            break

        # í…Œì´ë¸”ì˜ tr íƒœê·¸ì—ì„œ ë°ì´í„° í–‰ ì¶”ì¶œ
        table = soup.select_one('table.type_1')
        if not table:
            logger.warning(f"  [íŒŒì‹± ì‹¤íŒ¨] {code} page={page}: í…Œì´ë¸” ì—†ìŒ")
            break

        rows = table.select('tr')
        for row in rows:
            cols = row.select('td')
            if len(cols) < 6:
                continue

            date_text = cols[0].text.strip()
            close_text = cols[1].text.strip()

            # ë¹ˆ í–‰ ê±´ë„ˆë›°ê¸°
            if not date_text or not close_text:
                continue

            try:
                # ë‚ ì§œ íŒŒì‹± (YYYY.MM.DD â†’ YYYY-MM-DD)
                date_str = date_text.replace('.', '-').strip()
                # ìˆ«ì íŒŒì‹± (ì½¤ë§ˆ ì œê±°)
                close_val = float(close_text.replace(',', ''))
                change_text = cols[2].text.strip().replace(',', '')
                change_pct = cols[3].text.strip().replace('%', '').strip()
                volume_text = cols[4].text.strip().replace(',', '')
                trade_val_text = cols[5].text.strip().replace(',', '')

                # ì „ì¼ë¹„ì˜ ë¶€í˜¸ ê²°ì •: ë“±ë½ë¥ ë¡œ íŒë³„
                change_val = float(change_text) if change_text else 0.0
                pct_val = float(change_pct) if change_pct else 0.0
                if pct_val < 0:
                    change_val = -abs(change_val)

                all_rows.append({
                    'Date': date_str,
                    'Close': close_val,
                    'ì „ì¼ë¹„': change_val,
                    'ë“±ë½ë¥ ': pct_val,
                    'ê±°ë˜ëŸ‰': int(volume_text) if volume_text else 0,
                    'ê±°ë˜ëŒ€ê¸ˆ': int(trade_val_text) if trade_val_text else 0,
                    'ì‹œì¥': code,
                })
            except (ValueError, TypeError) as e:
                continue

        time.sleep(0.3)  # ì„œë²„ ë¶€í•˜ ë°©ì§€

    df = pd.DataFrame(all_rows)

    if not df.empty:
        # ë‚ ì§œ ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ + ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset='Date')
        df = df.sort_values('Date').reset_index(drop=True)
        logger.info(f"  -> {code} {len(df)}ì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    else:
        logger.warning(f"  -> {code} ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ (0ê±´)")

    return df


def scrape_all_indices(pages=10):
    """
    KOSPI + KOSDAQ ì§€ìˆ˜ë¥¼ ëª¨ë‘ ìˆ˜ì§‘í•˜ì—¬ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    Args:
        pages: ìˆ˜ì§‘í•  í˜ì´ì§€ ìˆ˜ (1í˜ì´ì§€ â‰ˆ 10ì¼)
    Returns:
        pandas DataFrame
    """
    session = create_session()

    df_kospi = scrape_index_daily("KOSPI", pages=pages, session=session)
    df_kosdaq = scrape_index_daily("KOSDAQ", pages=pages, session=session)

    result = pd.concat([df_kospi, df_kosdaq], ignore_index=True)
    return result


def save_index_data(df, directory=None):
    """ì§€ìˆ˜ ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if directory is None:
        directory = DATA_DIR
    os.makedirs(directory, exist_ok=True)

    today = datetime.now().strftime('%Y%m%d')
    filename = f"market_index_{today}.csv"
    filepath = os.path.join(directory, filename)

    df.to_csv(filepath, index=False, encoding='utf-8-sig')
    logger.info(f"  -> ì €ì¥ ì™„ë£Œ: {filepath} ({len(df)}ê±´)")
    return filepath


def load_index_data(directory=None):
    """
    data/ í´ë”ì—ì„œ ê°€ì¥ ìµœê·¼ì˜ market_index_*.csv ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ DataFrameì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if directory is None:
        directory = DATA_DIR

    if not os.path.exists(directory):
        return pd.DataFrame()

    # market_index_*.csv íŒŒì¼ ëª©ë¡ íƒìƒ‰
    files = sorted([
        f for f in os.listdir(directory)
        if f.startswith('market_index_') and f.endswith('.csv')
    ], reverse=True)

    if not files:
        return pd.DataFrame()

    filepath = os.path.join(directory, files[0])
    try:
        df = pd.read_csv(filepath, encoding='utf-8-sig')
        logger.info(f"  -> ë¡œë“œ ì™„ë£Œ: {filepath} ({len(df)}ê±´)")
        return df
    except Exception as e:
        logger.error(f"  -> ë¡œë“œ ì‹¤íŒ¨: {filepath}: {e}")
        return pd.DataFrame()


# ============================================================
# CLI ì‹¤í–‰
# ============================================================
if __name__ == '__main__':
    logger.info("=" * 50)
    logger.info("KOSPI / KOSDAQ ì§€ìˆ˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    logger.info("=" * 50)

    df = scrape_all_indices(pages=10)

    if not df.empty:
        save_index_data(df)
        print(f"\n[ì™„ë£Œ] ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df)}ê±´")
        print(df.head(10))
    else:
        print("[ì‹¤íŒ¨] ë°ì´í„° ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
